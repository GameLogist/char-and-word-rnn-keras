{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice's adventures in wonderland\n",
      "\n",
      "lewis carroll\n",
      "\n",
      "the millennium fulcrum edition 3.0\n",
      "\n",
      "chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of\n"
     ]
    }
   ],
   "source": [
    "raw_text = open('wonderland.txt').read()\n",
    "raw_text = raw_text.lower()\n",
    "print raw_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 144413\n",
      "Total Vocab: 45\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters:\", n_chars\n",
    "print \"Total Vocab:\", n_vocab\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alice's adventures in wonderland\\n\\nlewis carroll\\n\\nthe millennium fulcrum edition 3.0\\n\\nchapter i. down the rabbit-hole\\n\\nalice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought alice 'without pictures or\\nconversations?'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "sentences = tokenize.sent_tokenize(raw_text)\n",
    "lengths = []\n",
    "for i in xrange(len(sentences)):\n",
    "    lengths.append(len(sentences[i]))\n",
    "lengths = np.array(lengths)\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 144313\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in xrange(0, n_chars - seq_length):\n",
    "    seq_in  = raw_text[i: i+seq_length]\n",
    "    seq_out = raw_text[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print 'Total patterns:', n_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small LSTM Network : Constant Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reshape dataX to size of [samples, time steps, features] and scale it to 0-1\n",
    "# Represent dataY as one hot encoding\n",
    "X_train = np.reshape(dataX, (n_patterns, seq_length, 1))/float(n_vocab)\n",
    "Y_train = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 256)           264192      lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 45L)           11565       dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 275757\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.8974Epoch 00000: loss improved from inf to 2.89729, saving model to weights-improvement-00-2.8973.hdf5\n",
      "144313/144313 [==============================] - 929s - loss: 2.8973   \n",
      "Epoch 2/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.6795Epoch 00001: loss improved from 2.89729 to 2.67946, saving model to weights-improvement-01-2.6795.hdf5\n",
      "144313/144313 [==============================] - 918s - loss: 2.6795   \n",
      "Epoch 3/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.5777Epoch 00002: loss improved from 2.67946 to 2.57777, saving model to weights-improvement-02-2.5778.hdf5\n",
      "144313/144313 [==============================] - 913s - loss: 2.5778   \n",
      "Epoch 4/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.5084Epoch 00003: loss improved from 2.57777 to 2.50849, saving model to weights-improvement-03-2.5085.hdf5\n",
      "144313/144313 [==============================] - 905s - loss: 2.5085   \n",
      "Epoch 5/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.4443Epoch 00004: loss improved from 2.50849 to 2.44420, saving model to weights-improvement-04-2.4442.hdf5\n",
      "144313/144313 [==============================] - 903s - loss: 2.4442   \n",
      "Epoch 6/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.3870Epoch 00005: loss improved from 2.44420 to 2.38704, saving model to weights-improvement-05-2.3870.hdf5\n",
      "144313/144313 [==============================] - 905s - loss: 2.3870   \n",
      "Epoch 7/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.3312Epoch 00006: loss improved from 2.38704 to 2.33124, saving model to weights-improvement-06-2.3312.hdf5\n",
      "144313/144313 [==============================] - 902s - loss: 2.3312   \n",
      "Epoch 8/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.2780Epoch 00007: loss improved from 2.33124 to 2.27806, saving model to weights-improvement-07-2.2781.hdf5\n",
      "144313/144313 [==============================] - 902s - loss: 2.2781   \n",
      "Epoch 9/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.2292Epoch 00008: loss improved from 2.27806 to 2.22915, saving model to weights-improvement-08-2.2292.hdf5\n",
      "144313/144313 [==============================] - 899s - loss: 2.2292   \n",
      "Epoch 10/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.1818Epoch 00009: loss improved from 2.22915 to 2.18170, saving model to weights-improvement-09-2.1817.hdf5\n",
      "144313/144313 [==============================] - 898s - loss: 2.1817   \n",
      "Epoch 11/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.1409Epoch 00010: loss improved from 2.18170 to 2.14080, saving model to weights-improvement-10-2.1408.hdf5\n",
      "144313/144313 [==============================] - 898s - loss: 2.1408   \n",
      "Epoch 12/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.1014Epoch 00011: loss improved from 2.14080 to 2.10138, saving model to weights-improvement-11-2.1014.hdf5\n",
      "144313/144313 [==============================] - 898s - loss: 2.1014   \n",
      "Epoch 13/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.2557Epoch 00012: loss did not improve\n",
      "144313/144313 [==============================] - 901s - loss: 2.2556   \n",
      "Epoch 14/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.1127Epoch 00013: loss did not improve\n",
      "144313/144313 [==============================] - 898s - loss: 2.1127   \n",
      "Epoch 15/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0530Epoch 00014: loss improved from 2.10138 to 2.05286, saving model to weights-improvement-14-2.0529.hdf5\n",
      "144313/144313 [==============================] - 898s - loss: 2.0529   \n",
      "Epoch 16/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0321Epoch 00015: loss improved from 2.05286 to 2.03208, saving model to weights-improvement-15-2.0321.hdf5\n",
      "144313/144313 [==============================] - 898s - loss: 2.0321   \n",
      "Epoch 17/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0186Epoch 00016: loss improved from 2.03208 to 2.01845, saving model to weights-improvement-16-2.0185.hdf5\n",
      "144313/144313 [==============================] - 897s - loss: 2.0185   \n",
      "Epoch 18/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0189Epoch 00017: loss did not improve\n",
      "144313/144313 [==============================] - 897s - loss: 2.0189   \n",
      "Epoch 19/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0292Epoch 00018: loss did not improve\n",
      "144313/144313 [==============================] - 899s - loss: 2.0293   \n",
      "Epoch 20/20\n",
      "144256/144313 [============================>.] - ETA: 0s - loss: 2.0344Epoch 00019: loss did not improve\n",
      "144313/144313 [==============================] - 899s - loss: 2.0344   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xaf1966a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=20, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-16-2.0185.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" e\n",
      "to them, and all dripping wet, cross, and uncomfortable.\n",
      "\n",
      "the first question of course was, how to \"\n",
      " soenk at the could ao a lore of the sabbit sar and aalen and all the doere sf teered the had beee oo the daneer  and whet soeee to toink at ierse the saabit sareen the was aol a crea and an a lore of\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(X_train)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "print \"\\nGenerated Sequence:\"\n",
    "for i in xrange(200):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x/float(n_vocab)\n",
    "    prediction = model.predict(x)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small LSTM Network : Constant Characters With Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(dataX, (n_patterns, seq_length))/float(n_vocab)\n",
    "Y_train = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 100, 32)       1440        embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 256)           295936      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 45L)           11565       dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 308941\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, embedding_vector_length, input_length=seq_length))\n",
    "model1.add(LSTM(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144313/144313 [==============================] - 343s - loss: 3.0683   \n",
      "Epoch 2/10\n",
      "144313/144313 [==============================] - 344s - loss: 3.0407   \n",
      "Epoch 3/10\n",
      "144313/144313 [==============================] - 344s - loss: 3.0370   \n",
      "Epoch 4/10\n",
      "144313/144313 [==============================] - 343s - loss: 3.0347   \n",
      "Epoch 5/10\n",
      "144313/144313 [==============================] - 341s - loss: 3.0336   \n",
      "Epoch 6/10\n",
      "144313/144313 [==============================] - 341s - loss: 3.0331   \n",
      "Epoch 7/10\n",
      "144313/144313 [==============================] - 339s - loss: 3.0320   \n",
      "Epoch 8/10\n",
      "144313/144313 [==============================] - 337s - loss: 3.0319   \n",
      "Epoch 9/10\n",
      "144313/144313 [==============================] - 506s - loss: 3.0321   \n",
      "Epoch 10/10\n",
      "144313/144313 [==============================] - 574s - loss: 3.0319   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xaebbe2b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train, nb_epoch=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_countaskfqawf": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(X_train)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "for i in xrange(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern)))\n",
    "    x = x/float(n_vocab)\n",
    "    prediction = model1.predict(x)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep LSTM Network : Constant Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape dataX to size of [samples, time steps, features] and scale it to 0-1\n",
    "# Represent dataY as one hot encoding\n",
    "X_train = np.reshape(dataX, (n_patterns, seq_length, 1))/float(n_vocab)\n",
    "Y_train = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_14 (LSTM)                   (None, 100L, 256)     264192      lstm_input_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 100L, 256)     0           lstm_14[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                   (None, 64)            82176       dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 64)            0           lstm_15[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 45L)           2925        dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 349293\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
